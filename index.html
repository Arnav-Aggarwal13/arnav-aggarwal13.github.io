<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Arnav Aggarwal</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style>
    /* ---------- Layout + base styles ---------- */
    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      padding: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", system-ui, sans-serif;
      line-height: 1.6;
      background: #fafafa;
      color: #222;
    }

    .page {
      max-width: 1000px;
      margin: 0 auto;
      padding: 2rem 1.5rem 3rem;
    }

    a {
      color: #0070c9;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }

    /* ---------- Top navigation ---------- */
    .nav {
      display: flex;
      gap: 1.5rem;
      justify-content: flex-end;
      font-size: 0.9rem;
      margin-bottom: 1.5rem;
    }

    .nav a {
      text-transform: lowercase;
      letter-spacing: 0.08em;
    }

    .nav a.active {
      font-weight: 600;
    }

    /* ---------- Hero / header ---------- */
    .complete_header {
      display: flex;
      align-items: center;         /* vertically align hero + about text */
      justify-content: space-between;
      gap: 2rem;                   /* space between columns */
      margin-bottom: 3rem;
    }


    .hero {
      display: flex;
      flex-wrap: wrap;
      gap: 1.5rem;
      align-items: center;
      margin-bottom: 2rem;
      flex: 1 1 30%;
      justify-content: center;
      text-align: center;
    }

    .hero-img {
      width: 160px;
      height: 160px;
      border-radius: 50%;
      background: #ddd;
      object-fit: cover;
      flex-shrink: 0;
    }

    .hero-main h1 {
      margin: 0 0 0.25rem 0;
      font-size: 1.75rem;
    }

    .hero-main h2 {
      margin: 0 0 0.5rem 0;
      font-size: 1rem;
      font-weight: 500;
      color: #555;
    }

    .hero-main .status-line {
      font-size: 0.9rem;
      color: #555;
      margin: 0.15rem 0;
    }

    .hero-main .contact {
      margin-top: 0.5rem;
      font-size: 0.9rem;
    }

    .hero-main .contact span {
      margin-right: 0.5rem;
    }

    .about-me {
      flex: 1 1 60%;

    }

    /* ---------- Section headings ---------- */
    section {
      margin-top: 2rem;
    }

    .section-title {
      font-size: 1.6rem;
      margin-bottom: 0.75rem;
      font-weight: 600;
    }

    /* Add a leading "#" like in markdown sites */

    .section-layout {
      display: flex;
      align-items: center;         /* vertically align hero + about text */
      justify-content: space-between;
      gap: 2rem;                   /* space between columns */
      margin-bottom: 1rem;

    }
    .section-text {
      flex: 2 1 55%;
    }
    /* Image column on the right */
    .section-image {
      flex: 1 1 35%;
      max-width: 375px;      /* your width cap */
      display: flex;
      justify-content: center;

    }

    .section-image img {
      max-width: 100%;       /* ensure it can't exceed the container */
      max-height: 230px;     /* your height cap */
      width: auto;           /* allow width to shrink/grow */
      height: auto;          /* maintain aspect ratio */
      object-fit: contain;   /* NO cropping, scale inside box */
      border-radius: 0.75rem;
    }


    .deliverable-box {
      display: inline-block;
      padding: 4px 9px;
      background: #c0c0c0;
      color: #333;
      border: 1px solid #ccc;
      border-radius: 8px;
      font-size: 0.8rem;
      font-weight: 500;
      text-decoration: none;
      transition: background 0.2s, box-shadow 0.2s;
      margin-right: 8px;
    }

    .deliverable-box:hover {
      background: #a9a9a9;
      box-shadow: 0 2px 6px rgba(0,0,0,0.15);
    }
    


    /* ---------- Text blocks ---------- */
    .about-text {
      max-width: 60ch;
      font-size: 0.98rem;
    }

    .section-p {
      margin-top: 0.75em;
    }

    .entry {
      margin-bottom: 1.5rem;
    }

    .entry-title {
      font-size: 1rem;
      font-weight: 600;
      margin-bottom: 0.1rem;
    }

    .entry-meta {
      font-size: 0.9rem;
      color: #666;
      margin-bottom: 0.35rem;
    }

    .entry ul {
      margin: 0.25rem 0 0 1.25rem;
      padding: 0;
      font-size: 0.95rem;
    }

    .entry ul li + li {
      margin-top: 0.25rem;
    }

    /* ---------- News style (optional) ---------- */
    .news-item {
      margin-bottom: 0.6rem;
      font-size: 0.9rem;
    }

    .news-date {
      font-weight: 600;
      margin-right: 0.4rem;
    }

    /* ---------- Responsive tweaks ---------- */
    @media (max-width: 650px) {
      .nav {
        justify-content: center;
        flex-wrap: wrap;
      }
      .hero {
        justify-content: center;
        text-align: center;
      }
      .hero-main .contact {
        justify-content: center;
      }
    }
  </style>
</head>
<body>
  <div class="page">

    <!-- Top navigation -->
    <nav class="nav">
      <a href="#About">About</a>
      <a href="#Research">Research</a>

      <a href="#Projects">Projects</a>
      <a href="#Teaching">Teaching</a>
    </nav>

    <!-- Hero / header block -->
     <div class="complete_header">
      <header class="hero">
        <!-- Replace profile.jpg with your own photo file if you want -->
        <img src="images/profile.jpg" alt="Photo of Arnav Aggarwal" class="hero-img">

        <div class="hero-main">
          <h1>Arnav Aggarwal</h1>
          <h2>AI + Neuroscience @ MIT</h2>
      

          <!-- <p class="status-line"><strong>Currently:</strong> Doing research at the intersection of machine learning and neuroscience.</p>
          <p class="status-line">
            <strong>Previously:</strong> Machine Learning Intern @ LuxTronic · ErgOCR + LLMimic projects · TA for AI &amp; Rationality and Fundamentals of Programming.
          </p> -->

          <p class="contact">
            <span><a href="mailto:arnav813@mit.edu">arnav813@mit.edu</a></span>·
            <span><a href="https://www.linkedin.com/in/arnavaggs/" target="_blank" rel="noopener noreferrer">LinkedIn</a></span>
            <!-- TODO: add your GitHub link below if you want -->
            <!-- · <span><a href="https://github.com/YOUR_USERNAME" target="_blank" rel="noopener noreferrer">GitHub</a></span> -->
          </p>
        </div>
      </header>

      <!-- About -->
      <section id="About" class="about-me">
        <h2 class="section-title">About Me</h2>
        <p class="about-text">
          Hi! I’m an undergraduate at MIT studying Computer Science (AI &amp; Decision Making) with a minor in Brain and Cognitive Sciences. 
        </p>
        <p class="about-text">
          I work at the intersection of machine learning and neuroscience, where I aim to develop specialized machine learning approaches 
          to help answer core neuroscience questions—particularly those related to how we process visual information.
        </p>
        <p class="about-text">
          At the same time, I’m interested in using what we learn about human brain function and cognitive processes to build more 
          biologically realistic artificial systems. In particular, I’m drawn to models that can learn efficiently from limited data, 
          much like humans do, and that more faithfully capture the mechanisms underlying human perception.
        </p>
      </section>
    </div>

    <!-- Research -->
    <section id="Research" class="content-section">
      <h2 class="section-title">Research</h2>
      <div class="section-layout">
        <div class="section-text">
          <div class="entry">
            <div class="entry-title">Reconstructing EEG from fMRI</div>
            <div class="entry-meta">Lewis Lab · MIT · 2025–present</div>
            <p class="section-p">
              Traditionally, electroencephalograms (EEGs) are best suited for capturing activity with high temporal resolution, 
              whereas functional magnetic resonance imaging (fMRI) is more effective at capturing activity with high spatial resolution. 
              Correlating these two signals enables us to associate patterns of EEG activity with the specific brain regions responsible for them. 
              However, these signals are generally noisy, and their relationship is complex. To overcome this, I developed a novel encoder-decoder 
              neural network architecture to predict an EEG spectrogram from fMRI data, thereby modeling this relationship. We applied this model to sleep data. 
              Sleep is primarily understood through EEG. Our findings extend this understanding of sleep by tracing EEG features to specific brain regions that 
              control, generate, or are modulated by them. 
            </p>
          </div>
        </div>
        <div class="section-image"> 
          <!-- Optional image can go here -->
           <img src="images/eeg-fmri-figure.png" alt="EEG–fMRI reconstruction figure">
        </div>

      </div>

      <div class="section-layout">
        <div class="section-text"> 
          <div class="entry">
            <div class="entry-title">Human-aligned speech perception models</div>
            <div class="entry-meta">Lab for Computational Audition (McDermott Lab) · MIT · 2025</div>
            <a class="deliverable-box" href="deliverables/ARO-2026-Abstract.pdf" target="_blank">Abstract</a>
            <p class="section-p">
              Humans exhibit a remarkable ability to understand speech across various speakers and noisy environments. While there has been 
              significant progress in building accurate speech recognition models, these models rely on architectures that do not reflect brain processes, 
              leading to behavior that diverges from human behavior. In this lab, we built a non-recurrent speech recognition model and compared its alignment with 
              human speech perception against recurrent models. We found that non-recurrent models fail to reproduce patterns of human behavior compared to recurrent ones, 
              suggesting the importance of recurrence in human speech perception. We also examined the extent to which these models could 
              reproduce categorical perception. Categorical perception is the phenomenon in which humans perceive continuously varying speech sounds 
              as discrete categories. We discovered that the human-aligned speech recognition model showed the same unique 
              categorical perception pattern seen in humans. This work was accepted at the Association for Research in Otolaryngology (ARO). 
            </p>
          </div>
        </div>
        <div class="section-image">
            <img src="images/speech-perception-figure.png" alt="Speech models figure">
        </div>
      </div>
    </section>

    <!-- Projects / other work -->
    <section id="Projects">
      <h2 class="section-title">Projects</h2>
      <div class="section-layout">
        <div class="section-text">
          <div class="entry">
            <div class="entry-title">ErgOCR – rowing machine workout transcription</div>
            <div class="entry-meta">Personal project · 2025</div>
            <p class="section-p">
              In collegiate rowing, coxswains face a cumbersome and error-prone task of manually recording rowing machine (ergometer) scores, 
              often leading to inaccuracies potentially affecting athlete evaluation and boat placement. This project presents a computer vision 
              application designed to automate the extraction, aggregation, and recording of ergometer scores from images, thus improving efficiency 
              and accuracy compared with current manual efforts from coxswains. I achieved an accuracy of 99.5% of the scores in my test set. My developed 
              pipeline provides motivation for a broader range of OCR tasks on data stored in wireless tables. 
            </p>
          </div>
        </div>
        <div class="section-image">
            <img src="images/erg-figure.jpg" alt="ErgOCR figure">
        </div>
      </div>


      <div class="entry">
        <div class="entry-title">Machine Learning Intern · LuxTronic</div>
        <div class="entry-meta">Remote · Summer 2024</div>
        <p class="section-p">
          In my internship at LuxTronic, I worked on three main projects. (1) I upgraded object detection models to use 
          YOLOv10 that led to 5% (average) accuracy improvement on the most challenging customer datasets, while ensuring backward 
          compatibility for in-place upgrade. (2) I developed a CycleGAN to generate synthetic images of clear glass bottles from dark ones to 
          overcome the challenge of limited clear bottle data for model training. (3) I simplified client access to factory data by using GPT-4o 
          with RAG, allowing the clients to easily ask natural language queries and receive accuracte, comprehensible insights.
        </p>
      </div>
    </section>

    <!-- Teaching -->
    <section id="Teaching">
      <h2 class="section-title">Teaching</h2>

      <div class="entry">
        <div class="entry-title">Teaching Assistant · AI and Rationality (24.S00)</div>
        <div class="entry-meta">MIT · 2025</div>
        <p class="section-p">
          Covers concepts of rationality and agency, whether and in what sense artificial systems can have mental states, 
          probabilistic reasoning and learning from experience, decision-making under uncertainty, 
          sequential decision-making, and decision-making in multi-agent settings.
        </p>

      </div>

      <div class="entry">
        <div class="entry-title">Teaching Assistant · Fundamentals of Programming (6.1010)</div>
        <div class="entry-meta">MIT · 2024</div>
        <p class="section-p">
          Introduces fundamental concepts of programming. Designed to develop skills in applying basic methods from 
          programming languages to abstract problems. Topics include programming and Python basics, computational 
          concepts, software engineering, algorithmic techniques, data types, and recursion.
        </p> 
      </div>
    </section>

    

  </div>
</body>
</html>
